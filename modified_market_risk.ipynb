{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tTDuLAhCIfaZ"
   },
  
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhkffRZfIfab"
   },
   "source": [
    "# **Bidirectional Generative Adversarial Network (BiGAN) in Google Colab**\n",
    "\n",
    "In this notebook, we will implement a Bidirectional Generative Adversarial Network (BiGAN), an extension of the Generative Adversarial Network (GAN) that introduces an encoder along with the generator and discriminator. This encoder component enables the BiGAN to learn bidirectional mappings between the data space and latent space, making it beneficial for tasks in representation learning and feature extraction.\n",
    "\n",
    "---\n",
    "\n",
    "## **Overview of BiGAN Components**\n",
    "\n",
    "The BiGAN consists of three main components: the **Generator**, **Encoder**, and **Discriminator**.\n",
    "\n",
    "### 1. Generator\n",
    "The generator's role in the BiGAN is similar to its role in the traditional GAN. It maps samples from the latent space \\( z \\), typically drawn from a prior distribution, to the data space \\( x \\). The generator's output is a synthetic data sample, and it is optimized to produce realistic samples that can fool the discriminator.\n",
    "\n",
    "- **Objective**: \\( G(z|\\theta_G): z \\rightarrow x_{synthetic} \\)\n",
    "- **Purpose**: To create data samples that resemble real data.\n",
    "\n",
    "### 2. Encoder\n",
    "The encoder provides an inverse mapping of the generator, mapping data samples \\( x \\) to the latent space \\( z \\). This component enables the BiGAN to learn a bidirectional mapping between data space and latent space, bridging the gap between GANs and autoencoders.\n",
    "\n",
    "- **Objective**: \\( E(x|\\theta_E): x \\rightarrow z \\)\n",
    "- **Purpose**: To map real data samples back into the latent space, providing a feature-rich representation of data samples.\n",
    "\n",
    "### 3. Discriminator\n",
    "The discriminator is a classifier that distinguishes between real and synthetic samples. Unlike in a standard GAN, the BiGAN’s discriminator operates on pairs of data samples and encodings. Its task is twofold: it must discriminate between real and synthetic data pairs and between real and synthetic encodings.\n",
    "\n",
    "- **Objective**: \\( D(\\{x, z\\}|\\theta_D): \\{x, z\\} \\rightarrow [0, 1] \\)\n",
    "- **Positive Examples**: \\(\\{x, E(x|\\theta_E)\\}\\) (real data paired with real encodings)\n",
    "- **Negative Examples**: \\(\\{G(z|\\theta_G), z\\}\\) (synthetic data paired with latent samples)\n",
    "\n",
    "---\n",
    "\n",
    "## **BiGAN Objective Function and Optimization**\n",
    "\n",
    "The BiGAN is optimized by a min-max game, where the generator and encoder jointly aim to minimize the discriminator’s ability to classify real and synthetic pairs correctly. The discriminator tries to maximize its classification accuracy, while the generator and encoder attempt to minimize it. The objective function \\( V(D, G, E) \\) combines expectations over real and synthetic data distributions, ultimately minimizing the **Jensen-Shannon (JS) Divergence** between these distributions.\n",
    "\n",
    "### Objective Formulation\n",
    "\n",
    "\\[\n",
    "\\min_{G, E} \\max_{D} V(D(\\{x, z\\}), G(z), E(x))\n",
    "\\]\n",
    "\n",
    "The Jensen-Shannon Divergence between the real and synthetic data distributions is minimized, indicating that when the BiGAN reaches optimality, real and synthetic data distributions become indistinguishable.\n",
    "\n",
    "---\n",
    "\n",
    "## **Optimality and BiGAN’s Relation to Autoencoders**\n",
    "\n",
    "At the optimal solution, the generator and encoder become inverse functions of each other. This outcome enables a BiGAN to operate similarly to an autoencoder, with the generator functioning as a decoder. The BiGAN implicitly minimizes a reconstruction loss, aligning it with the goals of an autoencoder without requiring an explicit reconstruction loss function. This feature is especially beneficial for tasks that involve representation learning and feature extraction.\n",
    "\n",
    "### Key Points on Optimality\n",
    "\n",
    "1. **Bidirectional Mapping**: The encoder and generator become inverse mappings, learning a bidirectional representation of the data.\n",
    "2. **Implicit Reconstruction Loss**: The objective function of the generator and encoder indirectly minimizes reconstruction error, similar to an autoencoder, without explicitly defining a reconstruction loss.\n",
    "\n",
    "---\n",
    "\n",
    "## **Training the BiGAN**\n",
    "\n",
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhkffRZfIfab"
   },
   "source": [
    "# **Bidirectional Generative Adversarial Network (BiGAN) in Google Colab**\n",
    "\n",
    "In this notebook, we will implement a Bidirectional Generative Adversarial Network (BiGAN), an extension of the Generative Adversarial Network (GAN) that introduces an encoder along with the generator and discriminator. This encoder component enables the BiGAN to learn bidirectional mappings between the data space and latent space, making it beneficial for tasks in representation learning and feature extraction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JZFJkDFqI2vj"
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU, Dropout, Concatenate, BatchNormalization\n",
    "import matplotlib.pyplot as plt\n",
    "from plotnine import ggplot, aes, geom_density, geom_point, xlab, ylab, ggtitle, theme_minimal, ggthemes\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "gpu_devices = tf.config.list_physical_devices(device_type=\"GPU\")\n",
    "if gpu_devices:\n",
    "    print(\"GPU available\")\n",
    "    tf.config.experimental.set_memory_growth(device=gpu_devices[0], enable=True)\n",
    "else:\n",
    "    print(\"GPU not available. Using CPU.\")\n",
    "    tf.config.set_soft_device_placement(True)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU:\", gpu_devices)\n",
    "\n",
    "ret_market_data = pd.read_csv(filepath_or_buffer=\"/content/ret_market_data.csv\")\n",
    "mean = ret_market_data.mean()\n",
    "std = ret_market_data.std()\n",
    "ret_market_data = (ret_market_data - mean) / std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7-5V9i1rIfae"
   },
   "source": [
    "class BiGANDiscriminator(tf.keras.Model):\n",
    "    def __init__(self, num_hidden):\n",
    "        super().__init__()\n",
    "        self.concat = Concatenate(axis=-1)\n",
    "        self.feature_extractor = tf.keras.Sequential([\n",
    "            Dense(num_hidden, activation=LeakyReLU(0.2)),\n",
    "        ])\n",
    "        self.dropout = Dropout(0.5)\n",
    "        self.discriminator = Dense(1, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, x, z):\n",
    "        features = self.concat([x, z])\n",
    "        features = self.feature_extractor(features)\n",
    "        features = self.dropout(features)\n",
    "        return self.discriminator(features)\n",
    "\n",
    "class BiGANGenerator(tf.keras.Model):\n",
    "    def __init__(self, num_hidden, num_inputs):\n",
    "        super().__init__()\n",
    "        self.generator = tf.keras.Sequential([\n",
    "            Dense(num_hidden, activation=\"elu\"),\n",
    "            BatchNormalization(),\n",
    "            Dense(num_hidden, activation=\"elu\"),\n",
    "            BatchNormalization(),\n",
    "            Dense(num_inputs)\n",
    "        ])\n",
    "\n",
    "    def call(self, z):\n",
    "        return self.generator(z)\n",
    "\n",
    "class BiGANEncoder(tf.keras.Model):\n",
    "    def __init__(self, num_hidden, num_encoding):\n",
    "        super().__init__()\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            Dense(num_hidden, activation=LeakyReLU(0.2)),\n",
    "            BatchNormalization(),\n",
    "            Dense(num_hidden, activation=LeakyReLU(0.2)),\n",
    "            Dense(num_encoding, activation=\"tanh\")\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.encoder(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1cGudWjAIfaf",
    "scrolled": false
   },
   "source": [
    "num_inputs = ret_market_data.shape[1]\n",
    "num_hidden = 100\n",
    "num_encoding = 10\n",
    "num_epochs = 4000\n",
    "batch_size = 100\n",
    "generator = BiGANGenerator(num_hidden=num_hidden, num_inputs=num_inputs)\n",
    "discriminator = BiGANDiscriminator(num_hidden=num_hidden)\n",
    "encoder = BiGANEncoder(num_hidden=num_hidden, num_encoding=num_encoding)\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices(ret_market_data.values)\n",
    "ds = ds.shuffle(buffer_size=len(ret_market_data) * 2, reshuffle_each_iteration=True)\n",
    "ds = ds.batch(batch_size=batch_size)\n",
    "\n",
    "bce_loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "optimizer_disc = tf.keras.optimizers.RMSprop(learning_rate=2e-4, decay=1e-8)\n",
    "optimizer_enc_gen = tf.keras.optimizers.RMSprop(learning_rate=4e-4, decay=1e-8)\n",
    "disc_loss_metric = tf.keras.metrics.Mean(name=\"disc_loss\")\n",
    "enc_loss_metric = tf.keras.metrics.Mean(name=\"enc_loss\")\n",
    "gen_loss_metric = tf.keras.metrics.Mean(name=\"gen_loss\")\n",
    "\n",
    "@tf.function\n",
    "def train_step(x):\n",
    "    z = tf.random.uniform([x.shape[0], num_encoding], -1, 1)\n",
    "    with tf.GradientTape() as dis_tape, tf.GradientTape(persistent=True) as enc_gen_tape:\n",
    "        enc = encoder(x)\n",
    "        gen = generator(z)\n",
    "        disc_loss_real = bce_loss(tf.ones_like(x), discriminator(x, enc))\n",
    "        disc_loss_fake = bce_loss(tf.zeros_like(x), discriminator(gen, z))\n",
    "        disc_loss = 0.5 * (disc_loss_real + disc_loss_fake)\n",
    "        enc_loss = bce_loss(tf.zeros_like(x), discriminator(x, enc))\n",
    "        gen_loss = bce_loss(tf.ones_like(x), discriminator(gen, z))\n",
    "    gradients_disc = dis_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    optimizer_disc.apply_gradients(zip(gradients_disc, discriminator.trainable_variables))\n",
    "    disc_loss_metric(disc_loss)\n",
    "    gradients_enc = enc_gen_tape.gradient(enc_loss, encoder.trainable_variables)\n",
    "    optimizer_enc_gen.apply_gradients(zip(gradients_enc, encoder.trainable_variables))\n",
    "    enc_loss_metric(enc_loss)\n",
    "    gradients_gen = enc_gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    optimizer_enc_gen.apply_gradients(zip(gradients_gen, generator.trainable_variables))\n",
    "    gen_loss_metric(gen_loss)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    disc_loss_metric.reset_states()\n",
    "    enc_loss_metric.reset_states()\n",
    "    gen_loss_metric.reset_states()\n",
    "    for x in ds:\n",
    "        train_step(x)\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(f\"Epoch {epoch + 1}, Discriminator Loss: {disc_loss_metric.result()}, Encoder Loss: {enc_loss_metric.result()}, Generator Loss: {gen_loss_metric.result()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
