{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tTDuLAhCIfaZ"
   },
  
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhkffRZfIfab"
   },
   "source": [
    "# **Bidirectional Generative Adversarial Network (BiGAN) in Google Colab**\n",
    "\n",
    "In this notebook, we will implement a Bidirectional Generative Adversarial Network (BiGAN), an extension of the Generative Adversarial Network (GAN) that introduces an encoder along with the generator and discriminator. This encoder component enables the BiGAN to learn bidirectional mappings between the data space and latent space, making it beneficial for tasks in representation learning and feature extraction.\n",
    "\n",
    "---\n",
    "\n",
    "## **Overview of BiGAN Components**\n",
    "\n",
    "The BiGAN consists of three main components: the **Generator**, **Encoder**, and **Discriminator**.\n",
    "\n",
    "### 1. Generator\n",
    "The generator's role in the BiGAN is similar to its role in the traditional GAN. It maps samples from the latent space \\( z \\), typically drawn from a prior distribution, to the data space \\( x \\). The generator's output is a synthetic data sample, and it is optimized to produce realistic samples that can fool the discriminator.\n",
    "\n",
    "- **Objective**: \\( G(z|\\theta_G): z \\rightarrow x_{synthetic} \\)\n",
    "- **Purpose**: To create data samples that resemble real data.\n",
    "\n",
    "### 2. Encoder\n",
    "The encoder provides an inverse mapping of the generator, mapping data samples \\( x \\) to the latent space \\( z \\). This component enables the BiGAN to learn a bidirectional mapping between data space and latent space, bridging the gap between GANs and autoencoders.\n",
    "\n",
    "- **Objective**: \\( E(x|\\theta_E): x \\rightarrow z \\)\n",
    "- **Purpose**: To map real data samples back into the latent space, providing a feature-rich representation of data samples.\n",
    "\n",
    "### 3. Discriminator\n",
    "The discriminator is a classifier that distinguishes between real and synthetic samples. Unlike in a standard GAN, the BiGAN’s discriminator operates on pairs of data samples and encodings. Its task is twofold: it must discriminate between real and synthetic data pairs and between real and synthetic encodings.\n",
    "\n",
    "- **Objective**: \\( D(\\{x, z\\}|\\theta_D): \\{x, z\\} \\rightarrow [0, 1] \\)\n",
    "- **Positive Examples**: \\(\\{x, E(x|\\theta_E)\\}\\) (real data paired with real encodings)\n",
    "- **Negative Examples**: \\(\\{G(z|\\theta_G), z\\}\\) (synthetic data paired with latent samples)\n",
    "\n",
    "---\n",
    "\n",
    "## **BiGAN Objective Function and Optimization**\n",
    "\n",
    "The BiGAN is optimized by a min-max game, where the generator and encoder jointly aim to minimize the discriminator’s ability to classify real and synthetic pairs correctly. The discriminator tries to maximize its classification accuracy, while the generator and encoder attempt to minimize it. The objective function \\( V(D, G, E) \\) combines expectations over real and synthetic data distributions, ultimately minimizing the **Jensen-Shannon (JS) Divergence** between these distributions.\n",
    "\n",
    "### Objective Formulation\n",
    "\n",
    "\\[\n",
    "\\min_{G, E} \\max_{D} V(D(\\{x, z\\}), G(z), E(x))\n",
    "\\]\n",
    "\n",
    "The Jensen-Shannon Divergence between the real and synthetic data distributions is minimized, indicating that when the BiGAN reaches optimality, real and synthetic data distributions become indistinguishable.\n",
    "\n",
    "---\n",
    "\n",
    "## **Optimality and BiGAN’s Relation to Autoencoders**\n",
    "\n",
    "At the optimal solution, the generator and encoder become inverse functions of each other. This outcome enables a BiGAN to operate similarly to an autoencoder, with the generator functioning as a decoder. The BiGAN implicitly minimizes a reconstruction loss, aligning it with the goals of an autoencoder without requiring an explicit reconstruction loss function. This feature is especially beneficial for tasks that involve representation learning and feature extraction.\n",
    "\n",
    "### Key Points on Optimality\n",
    "\n",
    "1. **Bidirectional Mapping**: The encoder and generator become inverse mappings, learning a bidirectional representation of the data.\n",
    "2. **Implicit Reconstruction Loss**: The objective function of the generator and encoder indirectly minimizes reconstruction error, similar to an autoencoder, without explicitly defining a reconstruction loss.\n",
    "\n",
    "---\n",
    "\n",
    "## **Training the BiGAN**\n",
    "\n",
    "To train the BiGAN, we will implement the following steps:\n",
    "\n",
    "1. **Data Preparation**: Preprocess and prepare the real data samples.\n",
    "2. **Generator Training**: Map samples from the latent space \\( z \\) to data space \\( x \\).\n",
    "3. **Encoder Training**: Map data samples \\( x \\) back to latent space \\( z \\).\n",
    "4. **Discriminator Training**: Learn to distinguish between real and synthetic data-encoding pairs.\n",
    "5. **Optimization**: Use a min-max game to train the generator and encoder to minimize the discriminator's classification accuracy, effectively aligning real and synthetic distributions.\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "\n",
    "The BiGAN architecture enables bidirectional learning between data and latent spaces, providing a powerful framework for unsupervised feature learning. With its implicit reconstruction objective and its roots in GANs and autoencoders, the BiGAN is a versatile tool for representation learning, offering capabilities in both data generation and feature extraction.\n",
    "\n",
    "Let’s move forward and implement each component step-by-step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k3S2yCX9Ifac"
   },
   "source": [
    "# **Bidirectional Generative Adversarial Network (BiGAN) in Google Colab**\n",
    "\n",
    "In this notebook, we will implement a Bidirectional Generative Adversarial Network (BiGAN), an extension of the Generative Adversarial Network (GAN) that introduces an encoder along with the generator and discriminator. This encoder component enables the BiGAN to learn bidirectional mappings between the data space and latent space, making it beneficial for tasks in representation learning and feature extraction.\n",
    "\n",
    "---\n",
    "\n",
    "## **Overview of BiGAN Components**\n",
    "\n",
    "The BiGAN consists of three main components: the **Generator**, **Encoder**, and **Discriminator**.\n",
    "\n",
    "### 1. Generator\n",
    "The generator's role in the BiGAN is similar to its role in the traditional GAN. It maps samples from the latent space \\( z \\), typically drawn from a prior distribution, to the data space \\( x \\). The generator's output is a synthetic data sample, and it is optimized to produce realistic samples that can fool the discriminator.\n",
    "\n",
    "- **Objective**: \\( G(z|\\theta_G): z \\rightarrow x_{synthetic} \\)\n",
    "- **Purpose**: To create data samples that resemble real data.\n",
    "\n",
    "### 2. Encoder\n",
    "The encoder provides an inverse mapping of the generator, mapping data samples \\( x \\) to the latent space \\( z \\). This component enables the BiGAN to learn a bidirectional mapping between data space and latent space, bridging the gap between GANs and autoencoders.\n",
    "\n",
    "- **Objective**: \\( E(x|\\theta_E): x \\rightarrow z \\)\n",
    "- **Purpose**: To map real data samples back into the latent space, providing a feature-rich representation of data samples.\n",
    "\n",
    "### 3. Discriminator\n",
    "The discriminator is a classifier that distinguishes between real and synthetic samples. Unlike in a standard GAN, the BiGAN’s discriminator operates on pairs of data samples and encodings. Its task is twofold: it must discriminate between real and synthetic data pairs and between real and synthetic encodings.\n",
    "\n",
    "- **Objective**: \\( D(\\{x, z\\}|\\theta_D): \\{x, z\\} \\rightarrow [0, 1] \\)\n",
    "- **Positive Examples**: \\(\\{x, E(x|\\theta_E)\\}\\) (real data paired with real encodings)\n",
    "- **Negative Examples**: \\(\\{G(z|\\theta_G), z\\}\\) (synthetic data paired with latent samples)\n",
    "\n",
    "---\n",
    "\n",
    "## **BiGAN Objective Function and Optimization**\n",
    "\n",
    "The BiGAN is optimized by a min-max game, where the generator and encoder jointly aim to minimize the discriminator’s ability to classify real and synthetic pairs correctly. The discriminator tries to maximize its classification accuracy, while the generator and encoder attempt to minimize it. The objective function \\( V(D, G, E) \\) combines expectations over real and synthetic data distributions, ultimately minimizing the **Jensen-Shannon (JS) Divergence** between these distributions.\n",
    "\n",
    "### Objective Formulation\n",
    "\n",
    "\\[\n",
    "\\min_{G, E} \\max_{D} V(D(\\{x, z\\}), G(z), E(x))\n",
    "\\]\n",
    "\n",
    "The Jensen-Shannon Divergence between the real and synthetic data distributions is minimized, indicating that when the BiGAN reaches optimality, real and synthetic data distributions become indistinguishable.\n",
    "\n",
    "---\n",
    "\n",
    "## **Optimality and BiGAN’s Relation to Autoencoders**\n",
    "\n",
    "At the optimal solution, the generator and encoder become inverse functions of each other. This outcome enables a BiGAN to operate similarly to an autoencoder, with the generator functioning as a decoder. The BiGAN implicitly minimizes a reconstruction loss, aligning it with the goals of an autoencoder without requiring an explicit reconstruction loss function. This feature is especially beneficial for tasks that involve representation learning and feature extraction.\n",
    "\n",
    "### Key Points on Optimality\n",
    "\n",
    "1. **Bidirectional Mapping**: The encoder and generator become inverse mappings, learning a bidirectional representation of the data.\n",
    "2. **Implicit Reconstruction Loss**: The objective function of the generator and encoder indirectly minimizes reconstruction error, similar to an autoencoder, without explicitly defining a reconstruction loss.\n",
    "\n",
    "---\n",
    "\n",
    "## **Training the BiGAN**\n",
    "\n",
    "To train the BiGAN, we will implement the following steps:\n",
    "\n",
    "1. **Data Preparation**: Preprocess and prepare the real data samples.\n",
    "2. **Generator Training**: Map samples from the latent space \\( z \\) to data space \\( x \\).\n",
    "3. **Encoder Training**: Map data samples \\( x \\) back to latent space \\( z \\).\n",
    "4. **Discriminator Training**: Learn to distinguish between real and synthetic data-encoding pairs.\n",
    "5. **Optimization**: Use a min-max game to train the generator and encoder to minimize the discriminator's classification accuracy, effectively aligning real and synthetic distributions.\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "\n",
    "The BiGAN architecture enables bidirectional learning between data and latent spaces, providing a powerful framework for unsupervised feature learning. With its implicit reconstruction objective and its roots in GANs and autoencoders, the BiGAN is a versatile tool for representation learning, offering capabilities in both data generation and feature extraction.\n",
    "\n",
    "Let’s move forward and implement each component step-by-step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JZFJkDFqI2vj",
    "outputId": "783a724b-6989-4496-c4f8-cd8033478ff4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available. Using CPU.\n",
      "TensorFlow version: 2.17.0\n",
      "GPU: []\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn import linear_risk_model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import plotnine\n",
    "\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "gpu_devices = tf.config.list_physical_devices(device_type=\"GPU\")\n",
    "if gpu_devices:\n",
    "    print(\"GPU available\")\n",
    "    tf.config.experimental.set_memory_growth(device=gpu_devices[0], enable=True)\n",
    "    tf.keras.backend.set_floatx(value=\"float64\")\n",
    "    tf.config.run_functions_eagerly(run_eagerly=True)\n",
    "else:\n",
    "    print(\"GPU not available. Using CPU.\")\n",
    "    tf.config.set_soft_device_placement(True)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU:\", gpu_devices)\n",
    "\n",
    "ret_market_data = pd.read_csv(filepath_or_buffer=\"/content/ret_market_data.csv\")\n",
    "mean = ret_market_data.apply(func=np.mean, axis=0)\n",
    "std = ret_market_data.apply(func=np.std, axis=0)\n",
    "ret_market_data -= mean\n",
    "ret_market_data /= std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7-5V9i1rIfae"
   },
   "outputs": [],
   "source": [
    "class BidirectionalGenerativeAdversarialNetworkDiscriminator(tf.keras.Model):\n",
    "    def __init__(self, num_hidden):\n",
    "        super().__init__()\n",
    "\n",
    "        args, _, _, values = inspect.getargvalues(frame=inspect.currentframe())\n",
    "        values.pop(\"self\")\n",
    "\n",
    "        for arg, val in values.items():\n",
    "            setattr(self, arg, val)\n",
    "\n",
    "        self.concat = tf.keras.layers.Concatenate(axis=-1)\n",
    "        self.feature_extractor = tf.keras.Sequential(\n",
    "            layers=[\n",
    "                tf.keras.layers.Dense(\n",
    "                    units=self.num_hidden,\n",
    "                    activation=tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        self.dropout = tf.keras.layers.Dropout(rate=0.5)\n",
    "        self.discriminator = tf.keras.layers.Dense(\n",
    "            units=1,\n",
    "            activation=\"sigmoid\",\n",
    "        )\n",
    "\n",
    "    def call(self, x, z):\n",
    "        features = self.concat([x, z])\n",
    "        features = self.feature_extractor(features)\n",
    "        features = self.dropout(features)\n",
    "\n",
    "        return self.discriminator(features)\n",
    "\n",
    "class BidirectionalGenerativeAdversarialNetworkGenerator(tf.keras.Model):\n",
    "    def __init__(self, num_hidden, num_inputs):\n",
    "        super().__init__()\n",
    "\n",
    "        args, _, _, values = inspect.getargvalues(frame=inspect.currentframe())\n",
    "        values.pop(\"self\")\n",
    "\n",
    "        for arg, val in values.items():\n",
    "            setattr(self, arg, val)\n",
    "\n",
    "        self.generator = tf.keras.Sequential(\n",
    "            layers=[\n",
    "                tf.keras.layers.Dense(\n",
    "                    units=self.num_hidden,\n",
    "                    activation=\"elu\",\n",
    "                ),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Dense(\n",
    "                    units=self.num_hidden,\n",
    "                    activation=\"elu\",\n",
    "                ),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Dense(\n",
    "                    units=self.num_inputs,\n",
    "                    activation=\"linear\",\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def call(self, z):\n",
    "        return self.generator(z)\n",
    "\n",
    "\n",
    "class BidirectionalGenerativeAdversarialNetworkEncoder(tf.keras.Model):\n",
    "    def __init__(self, num_hidden, num_encoding):\n",
    "        super().__init__()\n",
    "\n",
    "        args, _, _, values = inspect.getargvalues(frame=inspect.currentframe())\n",
    "        values.pop(\"self\")\n",
    "\n",
    "        for arg, val in values.items():\n",
    "            setattr(self, arg, val)\n",
    "\n",
    "        self.encoder = tf.keras.Sequential(\n",
    "            layers=[\n",
    "                tf.keras.layers.Dense(\n",
    "                    units=self.num_hidden,\n",
    "                    activation=tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "                ),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Dense(\n",
    "                    units=self.num_hidden,\n",
    "                    activation=tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "                ),\n",
    "                tf.keras.layers.Dense(\n",
    "                    units=self.num_encoding,\n",
    "                    activation=\"tanh\",\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1cGudWjAIfaf",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_inputs = ret_market_data.shape[1]\n",
    "num_hidden = 100\n",
    "num_encoding = 10\n",
    "num_epochs = 4000\n",
    "batch_size = 100\n",
    "generator = BidirectionalGenerativeAdversarialNetworkGenerator(\n",
    "    num_hidden=num_hidden, num_inputs=num_inputs\n",
    ")\n",
    "discriminator = BidirectionalGenerativeAdversarialNetworkDiscriminator(\n",
    "    num_hidden=num_hidden\n",
    ")\n",
    "encoder = BidirectionalGenerativeAdversarialNetworkEncoder(\n",
    "    num_hidden=num_hidden, num_encoding=num_encoding\n",
    ")\n",
    "\n",
    "ds = (\n",
    "    tf.market_data.Dataset.from_tensor_slices(tensors=ret_market_data)\n",
    "    .shuffle(buffer_size=ret_market_data.shape[0] * 2, reshuffle_each_iteration=True)\n",
    "    .batch(batch_size=batch_size, drop_remainder=False)\n",
    ")\n",
    "\n",
    "reconstruction_loss = tf.keras.losses.BinaryCrossentropy(\n",
    "    from_logits=False, reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE\n",
    ")\n",
    "optimizer_disc = tf.keras.optimizers.RMSprop(\n",
    "    learning_rate=2e-4, decay=1e-8, clipvalue=1.0\n",
    ")\n",
    "optimizer_enc_gen = tf.keras.optimizers.RMSprop(\n",
    "    learning_rate=4e-4, decay=1e-8, clipvalue=1.0\n",
    ")\n",
    "disc_loss_metric = tf.keras.metrics.Mean(name=\"train_model_data_disc_loss\")\n",
    "enc_loss_metric = tf.keras.metrics.Mean(name=\"train_model_data_enc_loss\")\n",
    "gen_loss_metric = tf.keras.metrics.Mean(name=\"train_model_data_gen_loss\")\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_model_data_step(x, real, z, fake):\n",
    "    with tf.GradientTape() as dis_tape, tf.GradientTape(\n",
    "        persistent=True\n",
    "    ) as enc_gen_tape:\n",
    "        enc = encoder(x=x, train_model_dataing=True)\n",
    "        gen = generator(z=z, train_model_dataing=True)\n",
    "        disc_loss_real = reconstruction_loss(\n",
    "            y_true=real,\n",
    "            y_pred=discriminator(\n",
    "                x=x,\n",
    "                z=enc,\n",
    "                train_model_dataing=True,\n",
    "            ),\n",
    "        )\n",
    "        disc_loss_fake = reconstruction_loss(\n",
    "            y_true=fake,\n",
    "            y_pred=discriminator(\n",
    "                x=gen,\n",
    "                z=z,\n",
    "                train_model_dataing=True,\n",
    "            ),\n",
    "        )\n",
    "        disc_loss = 0.5 * (disc_loss_real + disc_loss_fake)\n",
    "        enc = encoder(x=x, train_model_dataing=True)\n",
    "        gen = generator(z=z, train_model_dataing=True)\n",
    "        enc_loss = reconstruction_loss(\n",
    "            y_true=fake,\n",
    "            y_pred=discriminator(\n",
    "                x=x,\n",
    "                z=enc,\n",
    "                train_model_dataing=True,\n",
    "            ),\n",
    "        )\n",
    "        gen_loss = reconstruction_loss(\n",
    "            y_true=real,\n",
    "            y_pred=discriminator(\n",
    "                x=gen,\n",
    "                z=z,\n",
    "                train_model_dataing=True,\n",
    "            ),\n",
    "        )\n",
    "    gradients_disc = dis_tape.gradient(\n",
    "        target=disc_loss,\n",
    "        sources=discriminator.train_model_dataable_variables,\n",
    "    )\n",
    "    optimizer_disc.apply_gradients(\n",
    "        grads_and_vars=zip(\n",
    "            gradients_disc,\n",
    "            discriminator.train_model_dataable_variables,\n",
    "        )\n",
    "    )\n",
    "    disc_loss_metric(disc_loss)\n",
    "    gradients_enc = enc_gen_tape.gradient(\n",
    "        target=enc_loss, sources=encoder.train_model_dataable_variables\n",
    "    )\n",
    "    optimizer_enc_gen.apply_gradients(\n",
    "        grads_and_vars=zip(\n",
    "            gradients_enc,\n",
    "            encoder.train_model_dataable_variables,\n",
    "        )\n",
    "    )\n",
    "    enc_loss_metric(enc_loss)\n",
    "    gradients_gen = enc_gen_tape.gradient(\n",
    "        target=gen_loss, sources=generator.train_model_dataable_variables\n",
    "    )\n",
    "    optimizer_enc_gen.apply_gradients(\n",
    "        grads_and_vars=zip(\n",
    "            gradients_gen,\n",
    "            generator.train_model_dataable_variables,\n",
    "        )\n",
    "    )\n",
    "    gen_loss_metric(gen_loss)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    disc_loss_metric.reset_state()\n",
    "    enc_loss_metric.reset_state()\n",
    "    gen_loss_metric.reset_state()\n",
    "\n",
    "    for x in ds:\n",
    "        train_model_data_step(\n",
    "            x=x,\n",
    "            real=np.ones(shape=(x.shape[0], 1)),\n",
    "            z=np.random.uniform(low=-1.0, high=1.0, size=(x.shape[0], num_encoding)),\n",
    "            fake=np.zeros(shape=(x.shape[0], 1)),\n",
    "        )\n",
    "\n",
    "    if ((epoch + 1) % 1000) == 0:\n",
    "        print(\"Epoch:\", epoch + 1)\n",
    "        print(\"Discriminator loss:\", disc_loss_metric.result())\n",
    "        print(\"Encoder loss:\", enc_loss_metric.result())\n",
    "        print(\"Generator loss:\", gen_loss_metric.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JZqReWLVIfaf"
   },
   "outputs": [],
   "source": [
    "num_sim = 1000\n",
    "with tf.device(device_name=\"/CPU:0\"):\n",
    "    x_mean = [\n",
    "        np.average(\n",
    "            a=(\n",
    "                generator(\n",
    "                    z=np.array(\n",
    "                        object=[\n",
    "                            np.random.uniform(low=-1.0, high=1.0, size=(num_encoding))\n",
    "                        ]\n",
    "                    )\n",
    "                )[0]\n",
    "                * std\n",
    "            )\n",
    "            + mean\n",
    "        )\n",
    "        for i in range(num_sim)\n",
    "    ]\n",
    "\n",
    "act_mean = [\n",
    "    np.average(a=(ret_market_data.iloc[i] * std) + mean) for i in range(ret_market_data.shape[0])\n",
    "]\n",
    "\n",
    "plotnine.options.figure_size = (12, 9)\n",
    "plot = (\n",
    "    plotnine.ggplot(\n",
    "        mapping=pd.melt(\n",
    "            frame=pd.concat(\n",
    "                objs=[\n",
    "                    pd.DataFrame(\n",
    "                        market_data=x_mean, columns=[\"BiGAN Portfolio Returns Distribution\"]\n",
    "                    ),\n",
    "                    pd.DataFrame(\n",
    "                        market_data=act_mean, columns=[\"Actual Portfolio Returns Distribution\"]\n",
    "                    ),\n",
    "                ],\n",
    "                axis=1,\n",
    "            ).reset_index(drop=True)\n",
    "        ).dropna(axis=\"index\")\n",
    "    )\n",
    "    + plotnine.geom_density(\n",
    "        mapping=plotnine.aes(\n",
    "            x=\"value\",\n",
    "            fill=\"factor(variable)\",\n",
    "        ),\n",
    "        alpha=0.5,\n",
    "        color=\"black\",\n",
    "    )\n",
    "    + plotnine.geom_point(\n",
    "        mapping=plotnine.aes(x=\"value\", y=0, fill=\"factor(variable)\"),\n",
    "        alpha=0.5,\n",
    "        color=\"black\",\n",
    "    )\n",
    "    + plotnine.xlab(xlab=\"Portfolio returns\")\n",
    "    + plotnine.ylab(ylab=\"Density\")\n",
    "    + plotnine.ggtitle(\n",
    "        title=\"Trained Bidirectional Generative Adversarial Network (BiGAN) Portfolio Returns\"\n",
    "    )\n",
    "    + plotnine.theme_matplotlib()\n",
    ")\n",
    "plot.save(filename=\"train_model_dataed_bigan_sampler.png\")\n",
    "print(\n",
    "    \"The VaR at 1% estimate given by the BiGAN: {}%\".format(\n",
    "        100 * np.percentile(a=x_mean, axis=0, q=1)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BROsAF7MIfag"
   },
   "outputs": [],
   "source": [
    "untrain_model_dataed_generator = BidirectionalGenerativeAdversarialNetworkGenerator(\n",
    "    num_hidden=num_hidden, num_inputs=num_inputs\n",
    ")\n",
    "\n",
    "untrain_model_dataed_x_mean = [\n",
    "    np.average(\n",
    "        a=(\n",
    "            untrain_model_dataed_generator(\n",
    "                z=np.array(\n",
    "                    object=[np.random.uniform(low=-1.0, high=1.0, size=(num_encoding))]\n",
    "                )\n",
    "            )[0]\n",
    "            * std\n",
    "        )\n",
    "        + mean\n",
    "    )\n",
    "    for i in range(num_sim)\n",
    "]\n",
    "\n",
    "plotnine.options.figure_size = (12, 9)\n",
    "plot = (\n",
    "    plotnine.ggplot(\n",
    "        mapping=pd.melt(\n",
    "            frame=pd.concat(\n",
    "                objs=[\n",
    "                    pd.DataFrame(\n",
    "                        market_data=untrain_model_dataed_x_mean,\n",
    "                        columns=[\"BiGAN Portfolio Returns Distribution\"],\n",
    "                    ),\n",
    "                    pd.DataFrame(\n",
    "                        market_data=act_mean, columns=[\"Actual Portfolio Returns Distribution\"]\n",
    "                    ),\n",
    "                ],\n",
    "                axis=1,\n",
    "            ).reset_index(drop=True)\n",
    "        ).dropna(axis=\"index\")\n",
    "    )\n",
    "    + plotnine.geom_density(\n",
    "        mapping=plotnine.aes(x=\"value\", fill=\"factor(variable)\"),\n",
    "        alpha=0.5,\n",
    "        color=\"black\",\n",
    "    )\n",
    "    + plotnine.geom_point(\n",
    "        mapping=plotnine.aes(x=\"value\", y=0, fill=\"factor(variable)\"),\n",
    "        alpha=0.5,\n",
    "        color=\"black\",\n",
    "    )\n",
    "    + plotnine.xlab(xlab=\"Portfolio returns\")\n",
    "    + plotnine.ylab(ylab=\"Density\")\n",
    "    + plotnine.ggtitle(\n",
    "        title=\"Untrain_model_dataed Bidirectional Generative Adversarial Network (BiGAN) Portfolio Returns\"\n",
    "    )\n",
    "    + plotnine.theme_matplotlib()\n",
    ")\n",
    "plot.save(filename=\"untrain_model_dataed_bigan_sampler.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "vNQq6evVIfai"
   },
   "source": [
    "## References\n",
    "\n",
    "1. [Understanding and Implementing Bidirectional GAN (BiGAN) in Deep Learning](https://towardsdatascience.com/understanding-and-implementing-bidirectional-gan-bigan-in-deep-learning-2a7a5a8e9b)\n",
    "2. Kingma, D. P., and Welling, M. (2014). *Auto-Encoding Variational Bayes*. [arXiv:1312.6114](https://arxiv.org/abs/1312.6114).\n",
    "3. Donahue, J., Krähenbühl, P., and Darrell, T. (2017). *Adversarial Feature Learning*. [arXiv:1605.09782](https://arxiv.org/abs/1605.09782).\n",
    "4. Radford, A., Metz, L., and Chintala, S. (2016). *Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks*. [arXiv:1511.06434](https://arxiv.org/abs/1511.06434).\n",
    "5. Dumoulin, V., and Bengio, Y. (2017). *Adversarially Learned Inference*. [arXiv:1606.00704](https://arxiv.org/abs/1606.00704).\n",
    "6. Goodfellow, I., Bengio, Y., and Courville, A. (2016). *Deep Learning*. MIT Press.\n",
    "7. Geron, A. (2019). *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow*. O'Reilly.\n",
    "8. Chollet, F. (2018). *Deep Learning with Python*. Manning Publications.\n",
    "9. Hull, John C. (2010). *Risk Management and Financial Institutions*. Pearson.\n",
    "10. [Keras Documentation](https://keras.io/guides/)\n",
    "11. [TensorFlow Guide](https://www.tensorflow.org/guide)\n",
    "12. [Scikit-Learn User Guide](https://scikit-learn.org/stable/user_guide.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yfUbq3_IL0OX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "author": "mes",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
